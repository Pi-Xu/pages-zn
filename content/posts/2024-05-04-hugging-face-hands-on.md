---
title: HuggingFace å­¦ä¹ ç¬”è®°
date: 2024-05-04T12:19:27+08:00
tags: [HuggingFace, CodeCheatSheet]
categories: [å­¦ä¹ ç¬”è®°]
math: true
---

è¿™éƒ¨åˆ†çš„æ–‡æ¡£ä¸»è¦ç”¨ä½œè®°å½•å­¦ä¹ , ä½¿ç”¨ HF çš„æ—¶å€™å¸¸ç”¨çš„ä¸€äº›æ“ä½œ, å¯èƒ½å¹¶ä¸ä¼šå¾ˆè¯¦ç»†, ä»¥åŠç¬¦åˆåˆå­¦è€…çš„éœ€æ±‚, ä½†æ˜¯å¯ä»¥å½“åš CheatSheet ä¸€ç±»æ–‡æ¡£ä½¿ç”¨.

## ç®¡é“çš„ä½¿ç”¨

### æ•´ä½“æµç¨‹: ä»¥æƒ…æ„Ÿåˆ†æä¸ºä¾‹

é¦–å…ˆ, ä» transformers ä¸­å¯¼å…¥ tokenizer, ä¸»è¦å°†æ–‡æœ¬è½¬æˆæ¨¡å‹å¯ä»¥è¯†åˆ«çš„ token:
```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

å…¶æ¬¡, å¯ä»¥å°†æ–‡æœ¬æ•°æ®å…ˆè½¬åŒ–ä¸ºå¯ä»¥è¯†åˆ«çš„ token
```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
```

è¾“å‡ºä¸€ä¸ªåŒ…å«ä¸¤ä¸ªé”®çš„å­—å…¸:
```
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
```

ä½¿ç”¨æ¨¡å‹ `AutoModel`:
```python
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
```

æœ€ç»ˆå…¶è¾“å‡ºä¸ºä¸€ä¸ª _éšè—çŠ¶æ€ï¼ˆhidden statesï¼‰_ï¼Œäº¦ç§° _ç‰¹å¾(features)_
Transformersæ¨¡å—çš„çŸ¢é‡è¾“å‡ºé€šå¸¸è¾ƒå¤§ã€‚å®ƒé€šå¸¸æœ‰ä¸‰ä¸ªç»´åº¦ï¼š
- **Batch size**: ä¸€æ¬¡å¤„ç†çš„åºåˆ—æ•°ï¼ˆåœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ä¸º2ï¼‰ã€‚
- **Sequence length**: åºåˆ—çš„æ•°å€¼è¡¨ç¤ºçš„é•¿åº¦ï¼ˆåœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ä¸º16ï¼‰ã€‚
- **Hidden size**: æ¯ä¸ªæ¨¡å‹è¾“å…¥çš„å‘é‡ç»´åº¦ã€‚

```python
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)

# è¾“å‡º: torch.Size([2, 16, 768])
```

Transformersæ¨¡å‹çš„è¾“å‡ºç›´æ¥å‘é€åˆ°æ¨¡å‹å¤´è¿›è¡Œå¤„ç†:

{{< figure src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg" title="æ¨¡å‹ç»“æ„" >}}

æœ‰å¦‚ä¸‹ç±»å‹çš„ _å¤´_
- `*Model`Â (retrieve the hidden states)
- `*ForCausalLM`
- `*ForMaskedLM`
- `*ForMultipleChoice`
- `*ForQuestionAnswering`
- `*ForSequenceClassification`
- `*ForTokenClassification`
- ä»¥åŠå…¶ä»– ğŸ¤—

### åŠ è½½æ¨¡å‹çš„æ–¹æ³•

ä»å¤´å¼€å§‹åŠ è½½:
```python
from transformers import BertConfig, BertModel

config = BertConfig()
model = BertModel(config)

# Model is randomly initialized!

print(config)

# è¾“å‡º:
# BertConfig {
#   [...]
#   "hidden_size": 768,
#   "intermediate_size": 3072,
#   "max_position_embeddings": 512,
#   "num_attention_heads": 12,
#   "num_hidden_layers": 12,
#   [...]
# }
```

åŠ è½½å·²ç»è®­ç»ƒå¥½çš„æ¨¡å‹ (ä¹Ÿå¯ä»¥ä½¿ç”¨ `AutoModel*` ç±»):
```python
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-cased")
```

ä¿å­˜æ¨¡å‹:
```python
model.save_pretrained("directory_on_my_computer")

# terminal
# ls directory_on_my_computer
# terminal è¾“å‡º:
# config.json pytorch_model.bin
```
### Tokenizer

å®ƒä»¬æœ‰ä¸€ä¸ªç›®çš„ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æ•°æ®ã€‚æ¨¡å‹åªèƒ½å¤„ç†æ•°å­—ï¼Œå› æ­¤æ ‡è®°å™¨(Tokenizer)éœ€è¦å°†æˆ‘ä»¬çš„æ–‡æœ¬è¾“å…¥è½¬æ¢ä¸ºæ•°å­—æ•°æ®ã€‚

ç›´æ¥è¿›è¡ŒåŠ è½½:
```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

ä½¿ç”¨:
```python
tokenizer("Using a Transformer network is simple")

# è¾“å‡º
# {'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
# 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
# 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

è¿›è¡Œç¼–ç çš„è¿‡ç¨‹:
```python
sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)

print(tokens)
# è¾“å‡º:['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']

ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
# è¾“å‡º: [7993, 170, 11303, 1200, 2443, 1110, 3014]
```

è§£ç :
```python
decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)
# è¾“å‡º: 'Using a Transformer network is simple'
```


### å¤šä¸ªåºåˆ—çš„å¤„ç†æ–¹å¼
- æˆ‘ä»¬å¦‚ä½•å¤„ç†å¤šä¸ªåºåˆ—ï¼Ÿ
- æˆ‘ä»¬å¦‚ä½•å¤„ç†å¤šä¸ªåºåˆ—ä¸åŒé•¿åº¦?
- è¯æ±‡ç´¢å¼•æ˜¯è®©æ¨¡å‹æ­£å¸¸å·¥ä½œçš„å”¯ä¸€è¾“å…¥å—ï¼Ÿ
- æ˜¯å¦å­˜åœ¨åºåˆ—å¤ªé•¿çš„é—®é¢˜ï¼Ÿ

åŸåˆ™1: æ¨¡å‹éœ€è¦ä¸€ä¸ª batch çš„è¾“å‡º. ä¹Ÿå°±æ˜¯è¯´æ¨¡å‹éœ€è¦çš„è¾“å…¥å½¢çŠ¶æ˜¯å¦‚ä¸‹æ‰€ç¤º
```python
# Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]
```

åŸåˆ™2: `attention_mask` çš„ä½œç”¨
```python
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)

# è¾“å‡º
# tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)
# tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)

batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
print(outputs.logits)

# è¾“å‡º(å¾—åˆ°ç›¸åŒçš„ç»“æœ)
# tensor([[ 1.5694, -1.3895],
#         [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
```

åŸåˆ™3: é•¿åºåˆ—éœ€è¦æˆªæ–­
> å¯¹äºTransformersæ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ¨¡å‹çš„åºåˆ—é•¿åº¦æ˜¯æœ‰é™çš„ã€‚å¤§å¤šæ•°æ¨¡å‹å¤„ç†å¤šè¾¾512æˆ–1024ä¸ªä»¤ç‰Œçš„åºåˆ—ï¼Œå½“è¦æ±‚å¤„ç†æ›´é•¿çš„åºåˆ—æ—¶ï¼Œä¼šå´©æºƒã€‚æ­¤é—®é¢˜æœ‰ä¸¤ç§è§£å†³æ–¹æ¡ˆï¼š
> - ä½¿ç”¨æ”¯æŒçš„åºåˆ—é•¿åº¦è¾ƒé•¿çš„æ¨¡å‹ã€‚
> - æˆªæ–­åºåˆ—ã€‚

### å¦‚ä½•ç»„åˆä¸Šè¿°çš„ä½¿ç”¨

```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

å¯ä»¥è®¾ç½®å‚æ•°è¡¥è¶³:
```python
# Will pad the sequences up to the maximum sequence length
model_inputs = tokenizer(sequences, padding="longest")

# Will pad the sequences up to the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# Will pad the sequences up to the specified max length
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

å¯ä»¥è®¾ç½®å‚æ•°æˆªæ–­:
```python
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Will truncate the sequences that are longer than the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# Will truncate the sequences that are longer than the specified max length
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

å¯ä»¥è®¾ç½®å‚æ•°è¿”å›ç‰¹å®šç±»å‹:
```python
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Returns PyTorch tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# Returns TensorFlow tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# Returns NumPy arrays
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```

ç›´æ¥ä½¿ç”¨ `Tokenizer()` (ç›´æ¥è°ƒç”¨æ ‡è®°å™¨(`Tokenizer`)å¯¹è±¡), å…¶ä¼šæ ¹æ®æ¨¡å‹æ·»åŠ  `[CLS]` ç­‰ç‰¹æ®Šå­—ç¬¦
### å¾®è°ƒ
#### å‡†å¤‡å¥½æ•°æ®é›†

å‡å¦‚æˆ‘ä»¬æœ‰ä¸€ä¸ª æ–‡æœ¬ æ•°æ®é›†:
```python
from transformers import AutoTokenizer
from datasets import load_dataset

checkpoint = "bert-base-uncased"
tokenizer =  AutoTokenizer.from_pretrained(checkpoint)

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets

# è¾“å‡º:
# DatasetDict({
#     train: Dataset({
#         features: ['sentence1', 'sentence2', 'label', 'idx'],
#         num_rows: 3668
#     })
#     validation: Dataset({
#         features: ['sentence1', 'sentence2', 'label', 'idx'],
#         num_rows: 408
#     })
#     test: Dataset({
#         features: ['sentence1', 'sentence2', 'label', 'idx'],
#         num_rows: 1725
#     })
# })

```

åœ¨ä¸Šä¸€èŠ‚ä¸­æˆ‘ä»¬çŸ¥é“åº”è¯¥å¦‚ä½•å°†é‡Œé¢çš„ sentence è½¬åŒ–ä¸º token:
```python
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

ç¼ºç‚¹æ˜¯è¿”å›å­—å…¸(å­—å…¸çš„é”®æ˜¯**è¾“å…¥è¯id(input_ids)**, Â **æ³¨æ„åŠ›é®ç½©(attention_mask)**Â å’ŒÂ **ç±»å‹æ ‡è®°ID(token_type_ids)**, å­—å…¸çš„å€¼æ˜¯é”®æ‰€å¯¹åº”å€¼çš„åˆ—è¡¨). è€Œä¸”åªæœ‰å½“æ‚¨åœ¨è½¬æ¢è¿‡ç¨‹ä¸­æœ‰**è¶³å¤Ÿçš„å†…å­˜**æ¥å­˜å‚¨æ•´ä¸ªæ•°æ®é›†æ—¶æ‰ä¸ä¼šå‡ºé”™

æ‰€ä»¥ä½¿ç”¨ä¸‹è¿°æ–¹æ³•:
```python
def tokenize_function(example):
    return (
	    tokenizer(example["sentence1"], 
				  example["sentence2"], truncation=True)
    )

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

## å¦‚ä½•ä½¿ç”¨ `PEFT` åº“

ä¸»è¦å‚è€ƒ: 
- [è‡ªå®šä¹‰å¾®è°ƒDemo](https://github.com/huggingface/peft/blob/main/examples/multilayer_perceptron/multilayer_perceptron_lora.ipynb)
- [å®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/peft/en/developer_guides/custom_models)

ä¸ºäº†ä½¿å¾—è°ƒç”¨çš„æ¥å£æ›´åŠ æ–¹ä¾¿, ä¸»è¦ä½¿ç”¨æ¯”è¾ƒåº•å±‚çš„æ¥å£, ä¸‹è¿°ä¸º training çš„è¿‡ç¨‹åŠå‚æ•°:
```python
lr = 0.002
batch_size = 64
max_epochs = 30
device = "cpu" if not torch.cuda.is_available() else "cuda"

def train(model, optimizer, criterion, train_dataloader, eval_dataloader, epochs):
    for epoch in range(epochs):
        model.train()
        train_loss = 0
        for xb, yb in train_dataloader:
            xb = xb.to(device)
            yb = yb.to(device)
            outputs = model(xb)
            loss = criterion(outputs, yb)
            train_loss += loss.detach().float()
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

        model.eval()
        eval_loss = 0
        for xb, yb in eval_dataloader:
            xb = xb.to(device)
            yb = yb.to(device)
            with torch.no_grad():
                outputs = model(xb)
            loss = criterion(outputs, yb)
            eval_loss += loss.detach().float()

        eval_loss_total = (eval_loss / len(eval_dataloader)).item()
        train_loss_total = (train_loss / len(train_dataloader)).item()
        print(f"{epoch=:<2}  {train_loss_total=:.4f}  {eval_loss_total=:.4f}")
```

å¦‚æœä½¿ç”¨ `PEFT`:
```python
config = peft.LoraConfig(
    r=8,
    target_modules=["seq.0", "seq.2"], # ä½¿ç”¨ LoRA è®­ç»ƒçš„å±‚æ•°, å‚è§ LoRA Config çš„å‚æ•°, ä¼šä½¿ç”¨regrex åŒ¹é…, https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/config.py
    modules_to_save=["seq.4"], # æœªä½¿ç”¨ LoRA è®­ç»ƒçš„å±‚æ•°
)

module = MLP().to(device)
module_copy = copy.deepcopy(module)  # we keep a copy of the original model for later
peft_model = peft.get_peft_model(module, config)
optimizer = torch.optim.Adam(peft_model.parameters(), lr=lr)
criterion = nn.CrossEntropyLoss()
peft_model.print_trainable_parameters()

# è¾“å‡º: trainable params: 56,164 || all params: 4,100,164 || trainable%: 1.369798866581922
```

æŸ¥çœ‹æ¯ä¸€å±‚çš„åç§°, å¯¹åº” `config` å°±å¯ä»¥çŸ¥é“å“ªäº›è¢«è®­ç»ƒäº†, å“ªäº›æœªè¢«è®­ç»ƒ, æ ¹æ®è¾“å‡º, å¯çŸ¥åªæœ‰å¾ˆå°‘ä¸€éƒ¨åˆ†è¢«è®­ç»ƒäº†
```python
[(n, type(m)) for n, m in MLP().named_modules()]
# è¾“å‡º:
[('', __main__.MLP),
 ('seq', torch.nn.modules.container.Sequential),
 ('seq.0', torch.nn.modules.linear.Linear),
 ('seq.1', torch.nn.modules.activation.ReLU),
 ('seq.2', torch.nn.modules.linear.Linear),
 ('seq.3', torch.nn.modules.activation.ReLU),
 ('seq.4', torch.nn.modules.linear.Linear),
 ('seq.5', torch.nn.modules.activation.LogSoftmax)]
```

è®­ç»ƒè¿‡ç¨‹:
```python
%time train(peft_model, optimizer, criterion, train_dataloader, eval_dataloader, epochs=max_epochs)
```

å…¶å® `PEFT` model å’Œ `base` model ç±»ä¼¼, `forward` çš„æ¥å£å’Œ torch éƒ½æ˜¯ç±»ä¼¼çš„. æ ¹æ®æ­¤éƒ¨åˆ†çš„å†…å®¹å¯ä»¥ç›´æ¥é€‚åº”åˆ°è®­ç»ƒæ¡†æ¶ä¸­.